{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihiFn_t_tBUo",
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span><ul class=\"toc-item\"><li><span><a href=\"#Очистка-данных\" data-toc-modified-id=\"Очистка-данных-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Очистка данных</a></span></li><li><span><a href=\"#Создание-токенов-и-их-лемматизация\" data-toc-modified-id=\"Создание-токенов-и-их-лемматизация-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Создание токенов и их лемматизация</a></span></li><li><span><a href=\"#Удаление-длинных-и-коротких-комментариев\" data-toc-modified-id=\"Удаление-длинных-и-коротких-комментариев-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Удаление длинных и коротких комментариев</a></span></li></ul></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span><ul class=\"toc-item\"><li><span><a href=\"#Трейн,Тест,-Валид\" data-toc-modified-id=\"Трейн,Тест,-Валид-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Трейн,Тест, Валид</a></span></li><li><span><a href=\"#Feature&amp;Target\" data-toc-modified-id=\"Feature&amp;Target-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Feature&amp;Target</a></span></li><li><span><a href=\"#LogisticRegression\" data-toc-modified-id=\"LogisticRegression-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>LogisticRegression</a></span></li><li><span><a href=\"#RidgeClassifier\" data-toc-modified-id=\"RidgeClassifier-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>RidgeClassifier</a></span></li><li><span><a href=\"#SVC\" data-toc-modified-id=\"SVC-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>SVC</a></span></li></ul></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FN8cMSjqtBUq"
   },
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iU6-9vYAtBUq"
   },
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5XQ3YmZtBUr"
   },
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "PldcTaN2tBUr",
    "outputId": "ff71226d-d9b7-4688-f3b3-31b703e972a9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Vladimir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Vladimir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Vladimir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "mport numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from tqdm import notebook\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import f1_score,precision_score, recall_score, precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from IPython.display import display\n",
    "from datetime import datetime \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6XflXcnetBUs"
   },
   "outputs": [],
   "source": [
    "comments = pd.read_csv(r'C:\\Users\\Vladimir\\Documents\\Job\\Course 13 text\\jupyter\\toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hlLDSl5jtBUt",
    "outputId": "1da78ede-0ecd-448e-a75d-445ec6a2b08f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159571 non-null  object\n",
      " 1   toxic   159571 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "comments.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4C-agssKtBUt"
   },
   "source": [
    "Почти 160 тысяч комментариев, пропусков нет, типы и названия столбцов нас устраивают."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dlxe8VejtBUt",
    "outputId": "2d6d8886-43fa-4d60-90d5-be2fe20a6235"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.898321\n",
      "1    0.101679\n",
      "Name: toxic, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def check_balance(df):\n",
    "    print(df['toxic'].value_counts(normalize=True))\n",
    "check_balance(comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0fWGsbvtBUt"
   },
   "source": [
    "Видим дисбаланс классов 90%-10%, нужно будет с этим что то делать. Будем менять баланс перед обучением моделей и только на трейне."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "5GGeE4m8tBUu"
   },
   "source": [
    "# cоздадим временный датасэт для модели\n",
    "df = comments.sample(1000).reset_index(drop=True)\n",
    "check_balance(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7IOsDJxtBUu"
   },
   "source": [
    "### Очистка данных\n",
    "Заметим наличие служебных символов, попробуем очистить их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "id": "wuNWMEB6tBUu",
    "outputId": "38851692-febc-4f7a-fa7d-293a27fe8518",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import string\n",
    "#string.punctuation\n",
    "def standardize_text(df, text_field):\n",
    "    df['text_orig'] = df[text_field]\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\\S+\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"[^A-Za-z0-9(),!?@\\']\", \" \")#\\n\\'\\`\\\"\\_\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\", \"at\")\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    return df\n",
    "#display(standardize_text(comments, \"text\"))\n",
    "df = comments.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SvM8Ef5tBUu"
   },
   "source": [
    "### Создание токенов и их лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5VQrcRgvtBUu"
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')#W+ means that either a word character (A-Za-z0-9_) or a dash (-) can go there.\n",
    "df[\"tokens\"] = df[\"text\"].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn = nltk.WordNetLemmatizer()\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.remove('not')\n",
    "snowball = SnowballStemmer(language=\"english\")\n",
    "ps = nltk.stem.porter.PorterStemmer()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "Rk3RJ_RLtBUv"
   },
   "source": [
    "def lemmatizing_stop_words(tokenized_text):\n",
    "    text = [wn.lemmatize(word) for word in text]\n",
    "    text = [word for word in text if word not in stop_words]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "df0['lemmas'] = df0['tokens'].apply(lambda x: lemmatizing_stop_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = []\n",
    "for token in df['tokens']:\n",
    "    text = [word for word in token if word not in stop_words]\n",
    "    text = [snowball.stem(word) for word in text]\n",
    "    text = ' '.join(text)\n",
    "    text_list.append(text)\n",
    "    del text\n",
    "df2 = pd.concat([df,pd.Series(text_list)],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.columns = ['text', 'toxic', 'tokens','lemmas']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Удаление длинных и коротких комментариев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_list = []\n",
    "for token in df2['tokens']:\n",
    "    count_list.append(len(token))\n",
    "count = pd.DataFrame(count_list,columns=['count'],index=df.index)\n",
    "df2 = pd.concat([df2,count],axis=1)\n",
    "#print(df.sort_values(by='count',ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Explanation, Why, the, edits, made, under, my...</td>\n",
       "      <td>explan whi edit made usernam hardcor metallica...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>[D, aww, He, matches, this, background, colour...</td>\n",
       "      <td>d aww he match background colour i seem stuck ...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Hey, man, I, m, really, not, trying, to, edit...</td>\n",
       "      <td>hey man i realli not tri edit war it guy const...</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[More, I, can, t, make, any, real, suggestions...</td>\n",
       "      <td>more i make real suggest improv i wonder secti...</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>[You, sir, are, my, hero, Any, chance, you, re...</td>\n",
       "      <td>you sir hero ani chanc rememb page</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[And, for, the, second, time, of, asking, when...</td>\n",
       "      <td>and second time ask view complet contradict co...</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[You, should, be, ashamed, of, yourself, That,...</td>\n",
       "      <td>you asham that horribl thing put talk page 128...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[Spitzer, Umm, theres, no, actual, article, fo...</td>\n",
       "      <td>spitzer umm there actual articl prostitut ring...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[And, it, looks, like, it, was, actually, you,...</td>\n",
       "      <td>and look like actual put speedi first version ...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "      <td>[And, I, really, don, t, think, you, understan...</td>\n",
       "      <td>and i realli think understand i came idea bad ...</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159525 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic  \\\n",
       "0       Explanation\\nWhy the edits made under my usern...      0   \n",
       "1       D'aww! He matches this background colour I'm s...      0   \n",
       "2       Hey man, I'm really not trying to edit war. It...      0   \n",
       "3       \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4       You, sir, are my hero. Any chance you remember...      0   \n",
       "...                                                   ...    ...   \n",
       "159566  \":::::And for the second time of asking, when ...      0   \n",
       "159567  You should be ashamed of yourself \\n\\nThat is ...      0   \n",
       "159568  Spitzer \\n\\nUmm, theres no actual article for ...      0   \n",
       "159569  And it looks like it was actually you who put ...      0   \n",
       "159570  \"\\nAnd ... I really don't think you understand...      0   \n",
       "\n",
       "                                                   tokens  \\\n",
       "0       [Explanation, Why, the, edits, made, under, my...   \n",
       "1       [D, aww, He, matches, this, background, colour...   \n",
       "2       [Hey, man, I, m, really, not, trying, to, edit...   \n",
       "3       [More, I, can, t, make, any, real, suggestions...   \n",
       "4       [You, sir, are, my, hero, Any, chance, you, re...   \n",
       "...                                                   ...   \n",
       "159566  [And, for, the, second, time, of, asking, when...   \n",
       "159567  [You, should, be, ashamed, of, yourself, That,...   \n",
       "159568  [Spitzer, Umm, theres, no, actual, article, fo...   \n",
       "159569  [And, it, looks, like, it, was, actually, you,...   \n",
       "159570  [And, I, really, don, t, think, you, understan...   \n",
       "\n",
       "                                                   lemmas  count  \n",
       "0       explan whi edit made usernam hardcor metallica...     50  \n",
       "1       d aww he match background colour i seem stuck ...     20  \n",
       "2       hey man i realli not tri edit war it guy const...     44  \n",
       "3       more i make real suggest improv i wonder secti...    114  \n",
       "4                      you sir hero ani chanc rememb page     14  \n",
       "...                                                   ...    ...  \n",
       "159566  and second time ask view complet contradict co...     46  \n",
       "159567  you asham that horribl thing put talk page 128...     21  \n",
       "159568  spitzer umm there actual articl prostitut ring...     11  \n",
       "159569  and look like actual put speedi first version ...     25  \n",
       "159570  and i realli think understand i came idea bad ...     34  \n",
       "\n",
       "[159525 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df2[(~(df2['count']>=1000) | (df2['count']<=1))]\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143338\n",
       "1     16187\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H21QFAfetBUv"
   },
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y107T0rUtBUv"
   },
   "source": [
    "### Трейн,Тест, Валид"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Y3qMFROUtBUv"
   },
   "outputs": [],
   "source": [
    "model,test = train_test_split(df2[['lemmas','toxic']],test_size=0.2,shuffle=True, random_state=890,stratify=df2['toxic'])\n",
    "train,valid = train_test_split(model,test_size=0.25,shuffle=True, random_state=890,stratify=model['toxic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yf8IUy0rtBUv"
   },
   "source": [
    "### Feature&Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "365pAw4gtBUv"
   },
   "outputs": [],
   "source": [
    "def feature_target(df):\n",
    "    X =  df['lemmas']\n",
    "    \n",
    "    y = df['toxic']\n",
    "    return X,y\n",
    "X,y = feature_target(df2)\n",
    "X_model,y_model = feature_target(model)\n",
    "X_train,y_train = feature_target(train.sample(10000))\n",
    "X_valid,y_valid = feature_target(valid.sample(10000))\n",
    "X_test,y_test   = feature_target(test)\n",
    "\n",
    "X_test = X_test.values.astype('U')\n",
    "X_model = X_model.values.astype('U')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-y33gQitBUw"
   },
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7NBRdiptBUw"
   },
   "source": [
    "Запустим гридсеч на трейне и подберем гиперпараметры модели. Прогнав несколько раз гридсерч пришло пониманию что необходимо варировать коэфициентом регуляризации, т.к. на гиперпараметрах\n",
    "<br> ``{'fit_intercept' : True, penalty': 'l2', 'solver': 'liblinear'}``\n",
    "<br>  Модель дает стабильно высокий результат. Причем значение С в районе 5-15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v8SgTg3ttBUw",
    "outputId": "6f678bd4-a4a9-431d-e126-c60387c90cf2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Значение метрики F1 на кросс-валидации равно 0.749162742876173\n"
     ]
    }
   ],
   "source": [
    "# Создадим словарь с парамет\n",
    "grid_params = {\n",
    "    # Key = step name from pipeline + __ + hyperparameter, value = tuple of possible values\n",
    "    #'logreg__penalty': ('l1', 'l2'),\n",
    "    'logreg__C': np.logspace(start=0.7,stop=1.5,num=5),\n",
    "    #'logreg__fit_intercept': (True,False),\n",
    "    #'logreg__solver':('liblinear','saga')\n",
    "}\n",
    "\n",
    "#Создадим последовательность для пайалайна\n",
    "estimators = [('TfIdf',TfidfVectorizer(stop_words=stop_words)),\n",
    "              ('logreg',LogisticRegression(penalty = 'l2',solver='liblinear',class_weight='balanced', random_state=42)   )\n",
    "             ]\n",
    "pipe = Pipeline(estimators)\n",
    "\n",
    "#GridSearch\n",
    "grid = GridSearchCV(pipe, n_jobs=-1, param_grid=grid_params,cv=5, scoring='f1')\n",
    "grid.fit(X_model, y_model) \n",
    "mean_scores = np.array(grid.cv_results_['mean_test_score'])\n",
    "print(\"Значение метрики F1 на кросс-валидации равно\", mean_scores.mean())\n",
    "#print(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RnLzN1httBUx"
   },
   "source": [
    "Нам удалось побить baseline на трэйне. Теперь трансформируем представление гиперпараметров, чтобы модель на тесте их поняла."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gt3L8v9ntBUx",
    "outputId": "05f58990-e31b-473b-eebd-3e64adb4551d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 7.943282347242813}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# преобазуем словарь с параметрами модели\n",
    "def transform_params(best_params):\n",
    "    _dict={}\n",
    "    for param in  best_params:\n",
    "        _dict[param.replace('logreg__','')]=best_params[param]\n",
    "    return _dict\n",
    "best_params = transform_params(grid.best_params_)    \n",
    "best_params    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Значение метрики F1 на трейне равно 0.763517359134889\n",
      "Время расчета пайплайна  7 секунд \n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now() \n",
    "estimators = [('TfIdf',TfidfVectorizer(stop_words=stop_words)),\n",
    "              ('logreg',LogisticRegression(C=8,penalty = 'l2',solver='liblinear',class_weight='balanced', random_state=42)   )\n",
    "             ]\n",
    "pipe2 = Pipeline(estimators)\n",
    "pipe2.fit(X_model, y_model)\n",
    "y_pred = pipe2.predict(X_test)\n",
    "print(\"Значение метрики F1 на трейне равно\",f1_score(y_test,y_pred))\n",
    "\n",
    "time_calc = (datetime.now() - start_time).seconds\n",
    "print('Время расчета пайплайна  {} секунд '.format(time_calc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpXuTd2ttBUx"
   },
   "source": [
    "На тесте нам  удалось побить бэйзлайн. Результат даже оказался лучше, чем на кросс валидации. Да и скорость данного алгоритма самая быстрая, по сравнению с другими."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CMjaCuNgtBUy"
   },
   "source": [
    "### RidgeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsJkLQOYtBUy"
   },
   "source": [
    "Попробуем линейную модель с регуляризацией. Исходя из экспериментов с логистической регрессией мы выберем тип регуляризации l2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kzLetsOmtBUz",
    "outputId": "a6f0e052-b2be-4c95-e431-ba75f25ecd1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Значение метрики F1 на кросс-валидации равно 0.2752911283245886\n",
      "Pipeline(steps=[('TfIdf',\n",
      "                 TfidfVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
      "                                             'our', 'ours', 'ourselves', 'you',\n",
      "                                             \"you're\", \"you've\", \"you'll\",\n",
      "                                             \"you'd\", 'your', 'yours',\n",
      "                                             'yourself', 'yourselves', 'he',\n",
      "                                             'him', 'his', 'himself', 'she',\n",
      "                                             \"she's\", 'her', 'hers', 'herself',\n",
      "                                             'it', \"it's\", 'its', 'itself', ...])),\n",
      "                ('ridge',\n",
      "                 RidgeClassifier(class_weight='balanced', random_state=42))])\n",
      "Время расчета пайплайна  28 секунд \n"
     ]
    }
   ],
   "source": [
    "#time\n",
    "start_time = datetime.now() \n",
    "\n",
    "# Создадим словарь с парамет\n",
    "grid_params = {\n",
    "    # Key = step name from pipeline + __ + hyperparameter, value = tuple of possible values\n",
    "    #'ridge__penalty': ('l1', 'l2'),\n",
    "    'ridge__alpha': [1,2,3,4],\n",
    "   # 'ridge__fit_intercept': (True,False),\n",
    "    #'ridge__solver':('sag','saga')\n",
    "}\n",
    "\n",
    "#Создадим последовательность для пайалайна\n",
    "estimators = [('TfIdf',TfidfVectorizer(stop_words=stop_words)),\n",
    "              ('ridge',RidgeClassifier(solver='sag',class_weight='balanced', random_state=42)   )\n",
    "             ]\n",
    "pipe = Pipeline(estimators)\n",
    "\n",
    "#GridSearch\n",
    "grid = GridSearchCV(pipe, n_jobs=-1, param_grid=grid_params,cv=5, scoring='f1')\n",
    "grid.fit(X_model, y_model)\n",
    "mean_scores = np.array(grid.cv_results_['mean_test_score'])\n",
    "print(\"Значение метрики F1 на кросс-валидации равно\", mean_scores.mean())\n",
    "print(pipe)\n",
    "\n",
    "\n",
    "#time\n",
    "time_calc = (datetime.now() - start_time).seconds\n",
    "print('Время расчета пайплайна  {} секунд '.format(time_calc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ridge__alpha': 4}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "thN2iJ2MtBUx"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:555: UserWarning: \"sag\" solver requires many iterations to fit an intercept with sparse inputs. Either set the solver to \"auto\" or \"sparse_cg\", or set a low \"tol\" and a high \"max_iter\" (especially if inputs are not standardized).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 0.6639869363145334\n"
     ]
    }
   ],
   "source": [
    "estimators = [('TfIdf',TfidfVectorizer(stop_words=stop_words,min_df=30)),\n",
    "              ('logreg',RidgeClassifier(alpha = 4,solver='sag',class_weight='balanced', random_state=42)   )\n",
    "             ]\n",
    "pipe = Pipeline(estimators)\n",
    "\n",
    "pipe.fit(X_model, y_model)\n",
    "y_pred = pipe.predict(X_test)\n",
    "print('Test',f1_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rTNgBNFttBUz"
   },
   "source": [
    "Линейная регрессия  подходит для решения данной задачи результат F1 в районе 0.66. Что тоже оч неплохо, но все же не дотягивает до бэйзлайна"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBZ4bL9EtBUz"
   },
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sPTf3laDtBUz",
    "outputId": "5d2c37d5-1465-4819-d839-885de9f54da7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Значение метрики F1 на кросс-валидации равно 0.34\n",
      "Pipeline(steps=[('TfIdf',\n",
      "                 TfidfVectorizer(stop_words=['i', 'me', 'my', 'myself', 'we',\n",
      "                                             'our', 'ours', 'ourselves', 'you',\n",
      "                                             \"you're\", \"you've\", \"you'll\",\n",
      "                                             \"you'd\", 'your', 'yours',\n",
      "                                             'yourself', 'yourselves', 'he',\n",
      "                                             'him', 'his', 'himself', 'she',\n",
      "                                             \"she's\", 'her', 'hers', 'herself',\n",
      "                                             'it', \"it's\", 'its', 'itself', ...])),\n",
      "                ('SVC', SVC(random_state=890))])\n",
      "Время расчета пайплайна  486 секунд \n"
     ]
    }
   ],
   "source": [
    "#time\n",
    "start_time = datetime.now() \n",
    "\n",
    "# Создадим словарь с парамет\n",
    "grid_params = {\n",
    "    #'SVC__kernel': ('linear', 'poly', 'rbf', 'sigmoid', 'precomputed'),\n",
    "    #'SVC__degree': [3,4,5,7],\n",
    "    'SVC__C': [5,7,9,11],\n",
    "    #'SVC__gamma':('auto','scale')\n",
    "}\n",
    "\n",
    "#Создадим последовательность для пайалайна\n",
    "estimators = [('TfIdf',TfidfVectorizer(stop_words=stop_words)),\n",
    "              ('SVC',SVC(random_state=890))\n",
    "             ]\n",
    "pipe = Pipeline(estimators)\n",
    "\n",
    "#GridSearch\n",
    "grid = GridSearchCV(pipe, n_jobs=-1, param_grid=grid_params,cv=5, scoring='f1')\n",
    "grid.fit(X_test, y_test)\n",
    "mean_scores = np.array(grid.cv_results_['mean_test_score'])\n",
    "print(\"Значение метрики F1 на кросс-валидации равно\", round(mean_scores.mean(),2))\n",
    "print(pipe)\n",
    "#time\n",
    "time_calc = (datetime.now() - start_time).seconds\n",
    "print('Время расчета пайплайна  {} секунд '.format(time_calc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 0.7721995094031071\n"
     ]
    }
   ],
   "source": [
    "estimators = [('TfIdf',TfidfVectorizer(stop_words=stop_words)),\n",
    "              ('SVC',SVC(kernel = 'linear',C=8,random_state=890))   \n",
    "             ]\n",
    "pipe = Pipeline(estimators)\n",
    "\n",
    "pipe.fit(X_model, y_model)\n",
    "y_pred = pipe.predict(X_test)\n",
    "print('Test',f1_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "loLkecw5tBUz"
   },
   "source": [
    "Очень неплохо на тесте показал себя метод опорных векторов 0.77 по метрике F1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdmv9SXKtBUz"
   },
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YK4eLGQftBUz"
   },
   "source": [
    "`Очистка данных`\n",
    "<br>На вход нам поступил датасэт со 160 тысячами комментариев, из которых 10% было размечено, как токсичные, а 90% как не токсичные. \n",
    "<br> С помощью регулярных выражений мы убрали служебные символы, кавычки, ссылки.\n",
    "Провели токенизацию и лемматизацию текстов из корпуса, перевели тексты в Юникод.\n",
    "\n",
    "Была попытка сделать downsampling, но это не привело к повышению точности модели. Поэтому было принято решение обучать на всем датасэте, используюя кросс-валидация и параметр взвешивания классов ``class_weight='balanced'``\n",
    "\n",
    "При проверке моделей на дефолтных параметрах было выяснено, что Логистическая регрессия дает лучший результат. Прогнав модель по сетке гиперпараметров мы увидели что лучший результат по метрике F1 (0.75) выходит с регуляризацией при параметрах {'fit_intercept' : True, penalty': 'l2', 'solver': 'liblinear'}. Но так же необходим была более тонкая настройка с подбором коэфициента регуляризации C. На тесте получили 0.76 \n",
    "\n",
    "Пробовали и линейные модели c Регуляризацией L2 (Ridge). Она показала очень  результат на метрике F1. 0.66\n",
    "\n",
    "Метод опорных веркторов (SVC) Так же не плохо справился с задачей, 0.77 на кросс валидации.\n",
    "\n",
    "У данной задачи есть потенциал по обработке текста и тюнингу гиперпараметров модели. Но в рамках данного проекта мы решили не развивать данные направления."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BwazUGQptBU0"
   },
   "source": [
    "## Чек-лист проверки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k792V6astBU0"
   },
   "source": [
    "- [x]  Jupyter Notebook открыт\n",
    "- [ ]  Весь код выполняется без ошибок\n",
    "- [ ]  Ячейки с кодом расположены в порядке исполнения\n",
    "- [ ]  Данные загружены и подготовлены\n",
    "- [ ]  Модели обучены\n",
    "- [ ]  Значение метрики *F1* не меньше 0.75\n",
    "- [ ]  Выводы написаны"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Копия блокнота \"13_NLP_wikiShop.ipynb\"",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
